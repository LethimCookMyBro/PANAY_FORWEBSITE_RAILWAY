# ========== Runtime ==========
APP_ENV=development
JWT_SECRET=replace-with-a-long-random-secret

# ========== Database & Ollama ==========
# Preferred: set DATABASE_URL directly
DATABASE_URL=postgresql://user:password@postgres:5432/plcnextdb
# Fallback (if DATABASE_URL is empty/placeholder): provide all PG vars below
# PGHOST=postgres
# PGPORT=5432
# PGUSER=user
# PGPASSWORD=password
# PGDATABASE=plcnextdb
# PGSSLMODE=require

OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama3.2

# LLM runtime budget (keep short in production to avoid 502/504 at edge)
LLM_TIMEOUT=20
LLM_MAX_RETRIES=1
CHAT_REQUEST_TIMEOUT_SECONDS=24
CHAT_EXECUTOR_WORKERS=4
LOAD_EMBEDDER_ON_DEMAND=false
CHAT_USE_RERANK=false

# ========== Storage layout (Railway volume) ==========
KNOWLEDGE_DIR=/data/Knowledge
MODEL_CACHE=/data/models
INGEST_STATE_PATH=/data/ingest/state.json

# Startup ingestion/seed policy
AUTO_EMBED_KNOWLEDGE=false
AUTO_EMBED_SYNC_IF_NOT_EMPTY=false
AUTO_EMBED_FORCE_RESCAN=false
# Safety guard: production blocks startup auto-ingest unless explicitly allowed.
ALLOW_STARTUP_INGEST_IN_PRODUCTION=false
AUTO_SEED_GOLDEN_QA=true
GOLDEN_QA_PATH=/data/Knowledge/golden_qa.json

# Optional startup ingest tuning (used only if AUTO_EMBED_KNOWLEDGE=true)
AUTO_EMBED_BATCH_SIZE=1000
AUTO_EMBED_CHUNK_SIZE=800
AUTO_EMBED_CHUNK_OVERLAP=150

# ========== Embeddings ==========
EMBED_MODEL=BAAI/bge-m3
# Device selection for embedding jobs: auto|cpu|cuda|cuda:0
EMBED_DEVICE=auto
# Hard token guard before embedding (prevents silent truncation on long chunks)
EMBED_MAX_TOKENS=480
EMBED_TOKEN_OVERLAP=64
# SentenceTransformer encode micro-batch size (lower on 4-8GB GPUs)
EMBED_ENCODE_BATCH_SIZE=8
# If CUDA becomes unstable mid-run, fallback to CPU and continue
EMBED_FALLBACK_TO_CPU_ON_CUDA_ERROR=true
# Docling parse speed/quality tradeoffs (for large text PDFs use false/true/false)
EMBED_DOCLING_OCR=true
EMBED_DOCLING_TABLE_STRUCTURE=true
EMBED_DOCLING_FORCE_BACKEND_TEXT=false
EMBED_DOCLING_IMAGES_SCALE=1.0
# Optional parser override: cpu|cuda
# If empty and EMBED_DEVICE is CUDA, ingest auto-pins Docling to CPU for stability.
# DOCLING_DEVICE=cpu
EVAL_EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
RAGAS_EMBED_MODEL_EVAL=sentence-transformers/all-MiniLM-L6-v2

# ========== RAGAS (LLM judge) ==========
ENABLE_RAGAS_LLM=true
RAGAS_LLM_PROVIDER=ollama
RAGAS_LLM_MODEL=phi3:mini
RAGAS_NUM_CTX=256
RAGAS_NUM_PREDICT=16
RAGAS_LLM_TEMPERATURE=0.0
RAGAS_TOP_K=1
RAGAS_TOP_P=0.05
RAGAS_REPEAT_PENALTY=1.05
RAGAS_CONTEXTS_K=3
RAGAS_CONTEXT_MAX_CHARS=240
RAGAS_TIMEOUT=120
RAGAS_BUDGET_S=110
RAGAS_MAX_WORKERS=1
DEFAULT_FAST_RAGAS=true
ENABLE_BACKGROUND_RAGAS=false
EVAL_WITH_RAGAS=true
ENABLE_CHAT_RAGAS=false

# Append deterministic source/page citations to final answer
APPEND_SOURCE_CITATIONS=true

# ========== Chunking ==========
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
MIN_CHUNK_LENGTH=30

# ========== Retrieval / Rerank ==========
USE_RERANK_DEFAULT=true
RETRIEVE_LIMIT=50
RERANK_CANDIDATES_MAX=32
RERANK_TOPN=8

# ========== OpenAI (optional) ==========
# OPENAI_API_KEY=sk-YOUR-SECRET-KEY-GOES-HERE
# OPENAI_MODEL=gpt-4o-mini
# RAGAS_LLM_PROVIDER=openai
